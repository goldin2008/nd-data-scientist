{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "# basic data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# scikit-learn modules for pipelining, transformation, model fitting and classification\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# nltk-modules for text processing, tokenizing and lemmatizing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download relevant ntlk packages\n",
    "nltk.download([\"punkt\", \"stopwords\", \"wordnet\"])\n",
    "\n",
    "# pickle for python object serialization and storing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///crisisresponse.db')\n",
    "df = pd.read_sql_table('messages', engine)\n",
    "X = df.loc[:,\"message\"]\n",
    "Y = df.iloc[:,4:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize, lemmatize, lower and remove punctuation of input text.\n",
    "\n",
    "    Input arguments:\n",
    "        text: Single string with input text \n",
    "              Example: 'For today:= this is, a advanced _ example #- String!'\n",
    "              \n",
    "    Output:\n",
    "        output: List of processed string\n",
    "                Example: ['today', 'advanced', 'example', 'string']\n",
    "        \n",
    "    \"\"\"\n",
    "    # set text to lower case and remove punctuation\n",
    "    text = re.sub(\"[\\W_]\", \" \", text)\n",
    "    text= text.lower()\n",
    "\n",
    "    # tokenize words \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # init and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    output = [lemmatizer.lemmatize(w) for w in tokens if not w in stop_words]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'advanced', 'example', 'string']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"For today:= this is, a advanced _ example #- String!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(n_estimators=100)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of all elements: \n",
      "\t X_train: (18219,)\n",
      "\t X_test: (7809,)\n",
      "\t Y_train: (18219, 36)\n",
      "\t Y_test: (7809, 36)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "print(\"\"\"\n",
    "Shape of all elements: \\n\\t X_train: {}\n",
    "\\t X_test: {}\n",
    "\\t Y_train: {}\n",
    "\\t Y_test: {}\"\"\".format(X_train.shape,\n",
    "                          X_test.shape,\n",
    "                          Y_train.shape,\n",
    "                          Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train pipeline\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pandas==1.0.4 in /opt/conda/lib/python3.6/site-packages (1.0.4)\r\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /opt/conda/lib/python3.6/site-packages (from pandas==1.0.4) (1.18.5)\r\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas==1.0.4) (2017.3)\r\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas==1.0.4) (2.6.1)\r\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas==1.0.4) (1.11.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pandas==1.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Functions for testing\n",
    "def flat_arr_df(two_d_data):\n",
    "    \"\"\"\n",
    "    Flatten array/list of arrays/lists and dataframes to lists.\n",
    "\n",
    "    Input arguments:\n",
    "        two_d_data: dataframe or array/list of arrays/lists \n",
    "                    Example: [[1,2,3],[4,5,6],[7,8,9]]\n",
    "              \n",
    "    Output:\n",
    "        flat_data: List of flattened Input\n",
    "                   Example: [1,2,3,4,5,6,7,8,9]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(two_d_data, (list, np.ndarray)):\n",
    "        if isinstance(two_d_data[0], (list, np.ndarray)):\n",
    "            flat_data = [item for sublist in two_d_data for item in sublist]\n",
    "        else:\n",
    "            print(\"Wrong datatype used, cannot flat this object\")\n",
    "            return \"\"\n",
    "    elif isinstance(two_d_data, pd.DataFrame):\n",
    "            flat_data = list(two_d_data.values.flatten())\n",
    "    \n",
    "    return flat_data\n",
    "\n",
    "def return_multioutput_f1_prec_recall(Y_pred, Y_test, mac_avg=False):\n",
    "    \"\"\"\n",
    "    Output classification report (f1, precision, recall) seperated for every category in\n",
    "    Multi_Output_Data\n",
    "\n",
    "    Input arguments:\n",
    "        Y_pred: dataframe or array/list of arrays/lists \n",
    "                    Example: [[1,2,3],[4,5,6],[7,8,9]]\n",
    "        \n",
    "        Y_test: dataframe or array/list of arrays/lists \n",
    "                    Example: [[1,2,3],[4,5,6],[7,8,9]]\n",
    "                    \n",
    "        mac_avg: If True returns Dict with Category names and F1 Score\n",
    "              \n",
    "    Output:\n",
    "        If mac_avg==False: prints precision, recall and f1-score\n",
    "        If mac_avg==True: returns Dict with Category names and F1 Score\n",
    "    \n",
    "    \"\"\"\n",
    "    if mac_avg:\n",
    "        mac_avg_dict = {}\n",
    "        \n",
    "    for pred_row, test_row, colname in zip(Y_pred.T, Y_test.to_numpy().T, Y_test.columns):\n",
    "        if mac_avg:\n",
    "            mac_avg_dict[colname] = classification_report(pred_row, test_row, output_dict=True)[\"macro avg\"][\"f1-score\"]\n",
    "        else:\n",
    "            print(\"Report for Category: {}\".format(colname))\n",
    "            print(classification_report(pred_row, test_row))    \n",
    "    \n",
    "    if mac_avg:\n",
    "        return mac_avg_dict\n",
    "    \n",
    "def return_flatted_f1_prec_recall(Y_pred, Y_test, mac_avg=False):\n",
    "    \"\"\"\n",
    "    Output classification report (f1, precision, recall) for flatted prediction and test data.\n",
    "\n",
    "    Input arguments:\n",
    "        Y_pred: dataframe or array/list of arrays/lists \n",
    "                    Example: [[1,2,3],[4,5,6],[7,8,9]]\n",
    "        \n",
    "        Y_test: dataframe or array/list of arrays/lists \n",
    "                    Example: [[1,2,3],[4,5,6],[7,8,9]]\n",
    "                    \n",
    "        mac_avg: If True returns F1-Score\n",
    "              \n",
    "    Output:\n",
    "        If mac_avg==False: prints precision recall and f1-score\n",
    "        If mac_avg==True: returns F1-Score only\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    flat_Y_pred = flat_arr_df(Y_pred)\n",
    "    flat_Y_test = flat_arr_df(Y_test)\n",
    "    if mac_avg:\n",
    "        return classification_report(flat_Y_pred, flat_Y_test, output_dict=True)[\"macro avg\"][\"f1-score\"]\n",
    "    else:\n",
    "        print(classification_report(flat_Y_pred, flat_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_arr = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "test_data_df = pd.DataFrame(test_data_arr)\n",
    "flat_arr_df(test_data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_arr_df(test_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00         1\n",
      "          2       1.00      1.00      1.00         1\n",
      "          3       1.00      1.00      1.00         1\n",
      "          4       1.00      1.00      1.00         1\n",
      "          5       1.00      1.00      1.00         1\n",
      "          6       1.00      1.00      1.00         1\n",
      "          7       1.00      1.00      1.00         1\n",
      "          8       1.00      1.00      1.00         1\n",
      "          9       1.00      1.00      1.00         1\n",
      "\n",
      "avg / total       1.00      1.00      1.00         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "return_flatted_f1_prec_recall(test_data_arr,test_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test real model prediction\n",
    "Y_pred_01 = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.96      0.97    264811\n",
      "          1       0.54      0.82      0.65     16313\n",
      "\n",
      "avg / total       0.96      0.95      0.95    281124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate over whole flattened prediction\n",
    "return_flatted_f1_prec_recall(Y_pred_01,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.6.14\n",
      "  latest version: 4.8.3\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - pandas\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2020.4.5.2 |       hecda079_0         147 KB  conda-forge\n",
      "    certifi-2020.4.5.2         |   py36h9f0ad1d_0         152 KB  conda-forge\n",
      "    pandas-0.24.2              |   py36hf484d3e_0        11.1 MB  conda-forge\n",
      "    python_abi-3.6             |          1_cp36m           4 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        11.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.6-1_cp36m\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                     2019.11.28-hecc5488_0 --> 2020.4.5.2-hecda079_0\n",
      "  certifi                                 2019.11.28-py36_0 --> 2020.4.5.2-py36h9f0ad1d_0\n",
      "  pandas                                      0.23.3-py36_0 --> 0.24.2-py36hf484d3e_0\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  scipy                                1.2.1-py36h09a28d5_1 --> 0.19.1-py36_blas_openblas_202\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "python_abi-3.6       | 4 KB      | ##################################### | 100% \n",
      "pandas-0.24.2        | 11.1 MB   | ##################################### | 100% \n",
      "certifi-2020.4.5.2   | 152 KB    | ##################################### | 100% \n",
      "ca-certificates-2020 | 147 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda update pandas -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-af327fa8b13e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate every Category on it's own\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcla_report_rfc_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_multioutput_f1_prec_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred_01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmac_avg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcla_report_rfc_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcla_report_rfc_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-7a30c975e1da>\u001b[0m in \u001b[0;36mreturn_multioutput_f1_prec_recall\u001b[0;34m(Y_pred, Y_test, mac_avg)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mmac_avg_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpred_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmac_avg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mmac_avg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"macro avg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f1-score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m                                              copy=copy, allow_dups=False)\n\u001b[1;32m   4375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_needs_reindex_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "# Evaluate every Category on it's own\n",
    "cla_report_rfc_b = pd.Series(return_multioutput_f1_prec_recall(Y_pred_01, Y_test, mac_avg=True))\n",
    "cla_report_rfc_b = cla_report_rfc_b.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-683560c0033b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate every Category on it's own\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcla_report_rfc_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_multioutput_f1_prec_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred_01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmac_avg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcla_report_rfc_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcla_report_rfc_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcla_report_rfc_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"F1 Score for RandomForestClassifier without Modification\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7a30c975e1da>\u001b[0m in \u001b[0;36mreturn_multioutput_f1_prec_recall\u001b[0;34m(Y_pred, Y_test, mac_avg)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mmac_avg_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpred_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmac_avg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mmac_avg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"macro avg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f1-score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "cla_report_rfc_b.rename(columns={0:\"F1 Score for RandomForestClassifier without Modification\"}, inplace=True)\n",
    "_ = cla_report_rfc_b.plot(kind=\"bar\", figsize=(15,5))\n",
    "_ = plt.title(\"Macro Averaged F1-Score for several Model Attempts\")\n",
    "_ = plt.ylabel(\"Macro Averaged F1-Score\")\n",
    "_ = plt.xlabel(\"Multi-Output Categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'vect', 'tfidf', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'clf__estimator__bootstrap', 'clf__estimator__class_weight', 'clf__estimator__criterion', 'clf__estimator__max_depth', 'clf__estimator__max_features', 'clf__estimator__max_leaf_nodes', 'clf__estimator__min_impurity_decrease', 'clf__estimator__min_impurity_split', 'clf__estimator__min_samples_leaf', 'clf__estimator__min_samples_split', 'clf__estimator__min_weight_fraction_leaf', 'clf__estimator__n_estimators', 'clf__estimator__n_jobs', 'clf__estimator__oob_score', 'clf__estimator__random_state', 'clf__estimator__verbose', 'clf__estimator__warm_start', 'clf__estimator', 'clf__n_jobs'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(n_estimators=100)))\n",
    "    ])\n",
    "\n",
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "        'clf__estimator__min_samples_split': [2, 3, 4],\n",
    "        'clf__estimator__n_estimators': [100, 200, 300],\n",
    "        'clf__estimator__criterion': ['entropy', 'gini']\n",
    "    }\n",
    "\n",
    "cv =  GridSearchCV(pipeline, param_grid=parameters, verbose=3, n_jobs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=100 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=100 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=100 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=200 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=200 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=200 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=300 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=300 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=300 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=3, clf__estimator__n_estimators=100 \n"
     ]
    }
   ],
   "source": [
    "cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_02 = cv.predict(X_test)\n",
    "return_flatted_f1_prec_recall(Y_pred_02,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cla_report_rfc_a = pd.Series(return_multioutput_f1_prec_recall(Y_pred_02, Y_test,mac_avg=True))\n",
    "cla_report_rfc_a = cla_report_rfc_a.to_frame()\n",
    "\n",
    "cla_report_rfc_a.rename(columns={0:\"F1 Score for RandomForestClassifier with Optimized Parameters\"}, inplace=True)\n",
    "cla_report_rfc_a = pd.concat([cla_report_rfc_b, cla_report_rfc_a], axis=1)\n",
    "_ = cla_report_rfc_a.plot(kind=\"bar\", figsize=(15,5))\n",
    "_ = plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "_ = plt.title(\"Macro Averaged F1-Score for several Model Attempts\")\n",
    "_ = plt.ylabel(\"Macro Averaged F1-Score\")\n",
    "_ = plt.xlabel(\"Multi-Output Categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/herrfeder/DataScientist/tree/master/Project_02_Disaster_Response_Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rfc_ct_pipe, open(\"rfc_ct_classifier.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
